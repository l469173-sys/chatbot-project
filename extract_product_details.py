import requests
from bs4 import BeautifulSoup
import json
import os
import re
import time
from urllib.parse import urljoin, urlparse

BASE_URL = "http://www.optimumopt.com/"
DATA_FILE = "crawled_data/data.json"
IMAGES_DIR = "crawled_data/images"

os.makedirs(IMAGES_DIR, exist_ok=True)

SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
})


def get_soup(url):
    """ç²å–ç¶²é å…§å®¹ä¸¦è§£æç‚º BeautifulSoup å°è±¡"""
    for attempt in range(3):
        try:
            response = SESSION.get(url, timeout=20)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or "utf-8"
            return BeautifulSoup(response.text, "lxml")
        except Exception as e:
            print(f"Error fetching {url} (attempt {attempt+1}/3): {e}")
            time.sleep(1.2)
    return None


def safe_filename(img_src: str) -> str | None:
    """å¾ img src è½‰æˆå®‰å…¨æª”åï¼ˆåªå– uploads å¾Œé¢çš„ basenameï¼‰"""
    if not img_src:
        return None

    img_src = img_src.split("?")[0].split("#")[0]
    img_src = re.sub(r"^/gen2/\d+/", "/", img_src)

    if "uploads/" not in img_src:
        return None

    tail = img_src.split("uploads/")[-1]
    tail = tail.replace("\\", "/")
    filename = os.path.basename(tail)  # âœ… åªå–æª”å
    if not filename:
        return None

    # åªæ¥å—åœ–ç‰‡å‰¯æª”å
    if not re.search(r"\.(png|jpg|jpeg|webp|gif)$", filename, re.IGNORECASE):
        return None

    return filename


def download_image(img_src: str, filename: str) -> bool:
    """ä¸‹è¼‰åœ–ç‰‡åˆ°æœ¬åœ° images è³‡æ–™å¤¾ï¼ˆimg_src å¯ç›¸å°è·¯å¾‘ï¼‰"""
    try:
        full_url = urljoin(BASE_URL, img_src)
        save_path = os.path.join(IMAGES_DIR, filename)

        if os.path.exists(save_path):
            return True

        resp = SESSION.get(full_url, timeout=30)
        resp.raise_for_status()
        with open(save_path, "wb") as f:
            f.write(resp.content)
        return True
    except Exception as e:
        print(f"  âŒ ä¸‹è¼‰å¤±æ•—: {img_src} -> {filename}: {e}")
        return False


def extract_product_detail(url):
    """å¾å•†å“é é¢æå–è©³ç´°è³‡è¨Š"""
    print(f"\næ­£åœ¨è™•ç†: {url}")
    soup = get_soup(url)
    if not soup:
        return None

    result = {"description": "", "images": [], "specifications": ""}

    detail_div = soup.find("div", {"id": "info-cnt-0"})
    if not detail_div:
        print("  âš ï¸  æœªæ‰¾åˆ°ç”¢å“è©³æƒ…å€åŸŸ (info-cnt-0)")
        return result

    # æè¿°ï¼ˆä¿ç•™ä½ çš„åšæ³•ï¼‰
    result["description"] = detail_div.get_text(strip=True, separator="\n")

    images = []
    seen = set()

    for img in detail_div.find_all("img"):
        src = img.get("src") or ""
        filename = safe_filename(src)
        if not filename:
            continue

        if filename in seen:
            continue
        seen.add(filename)

        # âœ… è‹¥æœ¬åœ°æ²’æœ‰ï¼Œå°±ä¸‹è¼‰
        if download_image(src, filename):
            images.append(filename)
            print(f"  âœ… åœ–ç‰‡: {filename}")

    result["images"] = images
    print(f"  å…±è™•ç† {len(images)} å¼µåœ–ç‰‡")
    print(f"  æè¿°é•·åº¦: {len(result['description'])} å­—å…ƒ")

    return result


def update_data_json():
    """æ›´æ–° data.json æ–‡ä»¶ï¼Œå¡«å……å•†å“æè¿°å’Œåœ–ç‰‡"""
    if not os.path.exists(DATA_FILE):
        print(f"éŒ¯èª¤: æ‰¾ä¸åˆ° {DATA_FILE}")
        return

    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    products = data.get("products", [])
    print(f"\nå…±æœ‰ {len(products)} å€‹å•†å“éœ€è¦è™•ç†\n")

    processed_urls = set()
    updated_count = 0

    for i, product in enumerate(products):
        url = product.get("url")
        if not url:
            continue

        if url in processed_urls:
            print(f"[{i+1}/{len(products)}] è·³éé‡è¤‡çš„å•†å“: {product.get('title', 'Unknown')}")
            continue
        processed_urls.add(url)

        print(f"\n{'='*60}")
        print(f"[{i+1}/{len(products)}] å•†å“: {product.get('title', 'Unknown')}")

        # å¦‚æœå·²ç¶“æœ‰æè¿°å’Œåœ–ç‰‡ï¼Œè·³é
        if product.get("description") and product.get("images"):
            print("  â„¹ï¸  æ­¤å•†å“å·²æœ‰è³‡æ–™ï¼Œè·³é...")
            continue

        details = extract_product_detail(url)
        if details is None:
            continue

        product["description"] = details.get("description", "")
        product["images"] = details.get("images", [])          # âœ… å­˜æª”å list
        product["specifications"] = details.get("specifications", "")

        updated_count += 1

        if updated_count % 5 == 0:
            with open(DATA_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"\n  ğŸ’¾ å·²ä¿å­˜é€²åº¦ ({updated_count} å€‹å•†å“å·²æ›´æ–°)")

        time.sleep(1)

    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*60}")
    print(f"âœ… å®Œæˆï¼å…±æ›´æ–°äº† {updated_count} å€‹å•†å“çš„è³‡æ–™")
    print(f"ğŸ“ è³‡æ–™å·²ä¿å­˜è‡³ {DATA_FILE}")


if __name__ == "__main__":
    update_data_json()
