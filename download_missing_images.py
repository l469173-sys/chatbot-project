import requests
from bs4 import BeautifulSoup
import json
import os
import re
from urllib.parse import urljoin, urlparse
import time

BASE_URL = "http://www.optimumopt.com/"
DATA_FILE = "crawled_data/data.json"
IMAGES_DIR = "crawled_data/images"

os.makedirs(IMAGES_DIR, exist_ok=True)

SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
})


def get_soup(url):
    """ç²å–ç¶²é å…§å®¹ä¸¦è§£æç‚º BeautifulSoup å°è±¡ï¼ˆå«ç°¡å–®é‡è©¦ï¼‰"""
    for attempt in range(3):
        try:
            resp = SESSION.get(url, timeout=20)
            resp.raise_for_status()
            resp.encoding = resp.apparent_encoding or "utf-8"
            return BeautifulSoup(resp.text, "lxml")
        except Exception as e:
            print(f"âŒ Error fetching {url} (attempt {attempt+1}/3): {e}")
            time.sleep(1.2)
    return None


def safe_filename_from_src(src: str) -> str | None:
    """
    å¾ src å–å¾—å®‰å…¨æª”åï¼š
    - ç§»é™¤ /gen2/æ•¸å­—/ å‰ç¶´
    - åªä¿ç•™ uploads/ å¾Œé¢çš„è·¯å¾‘
    - æœ€å¾Œåªå– basenameï¼ˆé¿å…å­è³‡æ–™å¤¾ï¼‰
    """
    if not src:
        return None

    src = src.split("?")[0].split("#")[0]
    src = re.sub(r"^/gen2/\d+/", "/", src)

    if "uploads/" not in src:
        return None

    tail = src.split("uploads/")[-1]
    tail = tail.replace("\\", "/")
    filename = os.path.basename(tail)

    if not filename:
        return None

    # åªæ¥å—å¸¸è¦‹åœ–ç‰‡å‰¯æª”å
    if not re.search(r"\.(png|jpg|jpeg|webp|gif)$", filename, re.IGNORECASE):
        return None

    return filename


def download_image(img_url: str, save_path: str) -> bool:
    """ä¸‹è¼‰åœ–ç‰‡åˆ°æŒ‡å®šè·¯å¾‘ï¼ˆimg_url å¯ç‚ºç›¸å°è·¯å¾‘ï¼‰"""
    try:
        full_url = urljoin(BASE_URL, img_url)
        resp = SESSION.get(full_url, timeout=30)
        resp.raise_for_status()
        with open(save_path, "wb") as f:
            f.write(resp.content)
        return True
    except Exception as e:
        print(f"    âŒ ä¸‹è¼‰å¤±æ•—: {img_url} - {e}")
        return False


def extract_and_download_images(url):
    """å¾å•†å“é é¢æå–åœ–ç‰‡ä¸¦ä¸‹è¼‰ç¼ºå°‘çš„åœ–ç‰‡ï¼ˆimages å­˜æª”åï¼‰"""
    print(f"\næ­£åœ¨è™•ç†: {url}")
    soup = get_soup(url)
    if not soup:
        return None

    result = {
        "description": "",
        "images": [],          # âœ… å­˜æª”å list: ["xxx.jpg", ...]
        "specifications": ""
    }

    detail_div = soup.find("div", {"id": "info-cnt-0"})
    if not detail_div:
        print("  âš ï¸  æœªæ‰¾åˆ°ç”¢å“è©³æƒ…å€åŸŸ (info-cnt-0)")
        return result

    # æè¿°ï¼šä¿ç•™åŸæœ¬åšæ³•ï¼Œä½†é¿å…è¶…å¤šç©ºç™½
    text = detail_div.get_text(separator="\n", strip=True)
    text = re.sub(r"\n{3,}", "\n\n", text)
    result["description"] = text

    images = []
    seen = set()

    for img in detail_div.find_all("img"):
        src = img.get("src") or ""
        filename = safe_filename_from_src(src)
        if not filename:
            continue

        if filename in seen:
            continue
        seen.add(filename)

        local_path = os.path.join(IMAGES_DIR, filename)

        if os.path.exists(local_path):
            images.append(filename)
            print(f"  âœ“ å·²å­˜åœ¨: {filename}")
            continue

        print(f"  â¬‡ï¸  ä¸‹è¼‰ä¸­: {filename}")
        if download_image(src, local_path):
            images.append(filename)
            print(f"  âœ… ä¸‹è¼‰æˆåŠŸ: {filename}")
        else:
            print(f"  âŒ ä¸‹è¼‰å¤±æ•—: {filename}")

    result["images"] = images
    print(f"  ğŸ“Š å…±è™•ç† {len(images)} å¼µåœ–ç‰‡")
    print(f"  ğŸ“ æè¿°é•·åº¦: {len(result['description'])} å­—å…ƒ")

    return result


def update_data_json():
    if not os.path.exists(DATA_FILE):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° {DATA_FILE}")
        return

    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    products = data.get("products", [])
    print(f"\n{'='*60}\nå…±æœ‰ {len(products)} å€‹å•†å“éœ€è¦è™•ç†\n{'='*60}\n")

    processed_urls = set()
    updated_count = 0
    downloaded_new_images = 0

    for i, product in enumerate(products):
        url = product.get("url")
        if not url:
            continue

        if url in processed_urls:
            print(f"[{i+1}/{len(products)}] â­ï¸  è·³éé‡è¤‡ URL: {product.get('title', 'Unknown')}")
            continue
        processed_urls.add(url)

        print(f"\n{'='*60}")
        print(f"[{i+1}/{len(products)}] å•†å“: {product.get('title', 'Unknown')}")

        # éœ€è¦æ›´æ–°ï¼šdescription ç©º æˆ– images ç©º
        needs_update = (not product.get("description")) or (not product.get("images"))
        if not needs_update:
            print("  â„¹ï¸  æ­¤å•†å“å·²æœ‰å®Œæ•´è³‡æ–™ï¼Œè·³é...")
            continue

        # å…ˆè¨˜éŒ„æ›´æ–°å‰å·²æœ‰å¤šå°‘åœ–ç‰‡æª”å
        old_images = product.get("images") or []
        if isinstance(old_images, str):
            try:
                old_images = json.loads(old_images)
            except Exception:
                old_images = []
        old_set = set(old_images) if isinstance(old_images, list) else set()

        details = extract_and_download_images(url)
        if details is None:
            continue

        product["description"] = details.get("description", "")
        product["images"] = details.get("images", [])          # âœ… æª”å list
        product["specifications"] = details.get("specifications", "")

        # åªç®—æ–°å¢åŠ çš„åœ–ç‰‡æ•¸ï¼ˆé¿å…çµ±è¨ˆå¤±çœŸï¼‰
        new_set = set(product["images"])
        downloaded_new_images += max(0, len(new_set - old_set))

        updated_count += 1

        if updated_count % 5 == 0:
            with open(DATA_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"\n  ğŸ’¾ å·²ä¿å­˜é€²åº¦ ({updated_count} å€‹å•†å“å·²æ›´æ–°)")

        time.sleep(1)

    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"\n{'='*60}")
    print("âœ… å®Œæˆï¼")
    print("ğŸ“Š çµ±è¨ˆè³‡è¨Šï¼š")
    print(f"  â€¢ æ›´æ–°äº† {updated_count} å€‹å•†å“çš„è³‡æ–™")
    print(f"  â€¢ æ–°ä¸‹è¼‰åœ–ç‰‡: {downloaded_new_images} å¼µ")
    print(f"ğŸ“ è³‡æ–™å·²ä¿å­˜è‡³ {DATA_FILE}")
    print(f"{'='*60}")


if __name__ == "__main__":
    update_data_json()
