import json
from urllib.parse import urlparse, urlunparse

DATA_PATH = "crawled_data/data.json"


def normalize_url(url: str) -> str:
    """
    å°‡ URL æ­£è¦åŒ–ï¼š
    - å»æ‰ query (?xxx)
    - å»æ‰ fragment (#xxx)
    - å»é™¤å°¾ç«¯ /
    """
    if not url:
        return ""

    p = urlparse(url)
    clean = urlunparse((
        p.scheme,
        p.netloc,
        p.path.rstrip("/"),
        "",  # query
        ""   # fragment
    ))
    return clean


with open(DATA_PATH, "r", encoding="utf-8") as f:
    data = json.load(f)

products = data.get("products", [])
original_count = len(products)

print(f"åŸå§‹å•†å“æ•¸é‡: {original_count}")

seen_urls = set()
unique_products = []
removed = []

for product in products:
    raw_url = product.get("url", "")
    norm_url = normalize_url(raw_url)

    # URL ç‚ºç©ºï¼šä¿ç•™ï¼Œä½†ä¸åƒèˆ‡å»é‡
    if not norm_url:
        unique_products.append(product)
        continue

    if norm_url not in seen_urls:
        seen_urls.add(norm_url)
        product["url"] = norm_url   # é †ä¾¿å›å¯«ä¹¾æ·¨ URL
        unique_products.append(product)
    else:
        removed.append(product)
        print(f"ğŸ—‘ï¸ ç§»é™¤é‡è¤‡å•†å“: {product.get('title', 'Unknown')}")

data["products"] = unique_products

with open(DATA_PATH, "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)

print("\n" + "=" * 50)
print(f"å»é‡å¾Œå•†å“æ•¸é‡: {len(unique_products)}")
print(f"å¯¦éš›ç§»é™¤é‡è¤‡å•†å“: {len(removed)}")
print("=" * 50)

print("\nğŸ“‹ å»é‡å¾Œå•†å“åˆ—è¡¨ï¼š")
for i, product in enumerate(unique_products, 1):
    title = product.get("title", "Unknown")
    category = product.get("category", "æœªåˆ†é¡")
    print(f"{i}. {title} ({category})")
