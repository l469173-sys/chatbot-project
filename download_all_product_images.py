import os
import re
import json
import time
import hashlib
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup


BASE_URL = "http://www.optimumopt.com/"
DATA_FILE = "crawled_data/data.json"
IMAGES_DIR = "crawled_data/images"

os.makedirs(IMAGES_DIR, exist_ok=True)

S = requests.Session()
S.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
})


def get_soup(url: str) -> BeautifulSoup | None:
    for attempt in range(3):
        try:
            r = S.get(url, timeout=20)
            r.raise_for_status()
            r.encoding = r.apparent_encoding or "utf-8"
            return BeautifulSoup(r.text, "lxml")
        except Exception as e:
            print(f"âŒ Error fetching {url} (attempt {attempt+1}/3): {e}")
            time.sleep(1.2)
    return None


def safe_filename_from_url(full_url: str) -> str:
    """æŠŠ URL è®Šæˆå®‰å…¨æª”åï¼ˆé¿å…åŒ…å« / é€ æˆå­è³‡æ–™å¤¾ï¼‰"""
    path = urlparse(full_url).path
    name = os.path.basename(path)

    if not name:
        name = hashlib.md5(full_url.encode("utf-8")).hexdigest() + ".jpg"

    # å»é™¤ query æ®˜ç•™
    name = name.split("?")[0].split("#")[0]

    # è‹¥æª”åä»å«å¥‡æ€ªå­—å…ƒï¼Œåšç°¡åŒ–
    name = re.sub(r"[^A-Za-z0-9._-]", "_", name)

    # è‹¥æ²’å‰¯æª”åï¼Œè£œ jpg
    if not re.search(r"\.(png|jpg|jpeg|webp|gif)$", name, re.IGNORECASE):
        name += ".jpg"

    return name


def download_image(full_img_url: str) -> str | None:
    """ä¸‹è¼‰åœ–ç‰‡ä¸¦å›å‚³ã€æª”åã€"""
    try:
        filename = safe_filename_from_url(full_img_url)
        save_path = os.path.join(IMAGES_DIR, filename)

        if os.path.exists(save_path):
            return filename

        r = S.get(full_img_url, timeout=30)
        r.raise_for_status()

        with open(save_path, "wb") as f:
            f.write(r.content)

        return filename
    except Exception as e:
        print(f"    âŒ ä¸‹è¼‰å¤±æ•—: {full_img_url} - {e}")
        return None


def clean_text(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    return s


def split_desc_specs(raw_text: str) -> tuple[str, str]:
    """
    å˜—è©¦æŠŠå…§å®¹åˆ‡æˆæè¿°/è¦æ ¼ã€‚
    - æ‰¾åˆ°ã€è¦æ ¼/Specificationsã€ç­‰é—œéµå­—ï¼Œå¾ŒåŠç•¶è¦æ ¼
    """
    raw = (raw_text or "").strip()
    if not raw:
        return "", ""

    markers = ["ç”¢å“è¦æ ¼", "è¦æ ¼", "Specifications", "Specification", "SPEC"]
    idx = None
    lower = raw.lower()
    for m in markers:
        p = lower.find(m.lower())
        if p != -1 and p > 20:
            idx = p
            break

    if idx is None:
        return clean_text(raw), ""

    return clean_text(raw[:idx]), raw[idx:].strip()


def extract_and_download_all_images(url: str) -> dict | None:
    print(f"\næ­£åœ¨è™•ç†: {url}")
    soup = get_soup(url)
    if not soup:
        return None

    result = {"description": "", "images": [], "specifications": ""}

    images: list[str] = []

    # ===== 1) ä¸»åœ–ï¼šä¿ç•™ä½ åŸæœ¬é‚è¼¯ï¼ˆwidth=400ï¼‰ï¼Œä½†æ›´ä¿å®ˆ =====
    print("  ğŸ” å°‹æ‰¾å•†å“ä¸»åœ–...")
    main_img = soup.find("img", {"width": "400"})
    if main_img:
        src = main_img.get("src") or ""
        if src:
            full_img_url = urljoin(BASE_URL, src)
            fn = download_image(full_img_url)
            if fn and fn not in images:
                images.append(fn)
                print(f"  âœ… ä¸»åœ–: {fn}")
    else:
        print("  âš ï¸  æœªæ‰¾åˆ°å•†å“ä¸»åœ– (width=400)")

    # ===== 2) è©³æƒ…å€å¡Šï¼šinfo-cnt-0ï¼ˆä½ åŸæœ¬çš„ï¼‰ =====
    print("  ğŸ” å°‹æ‰¾ç”¢å“è©³æƒ…å€åŸŸ...")
    detail_div = soup.find("div", {"id": "info-cnt-0"})

    # è‹¥æ‰¾ä¸åˆ°ï¼Œé€€è€Œæ±‚å…¶æ¬¡æŠ“ block-body
    if not detail_div:
        detail_div = soup.find("div", {"id": "block-body"}) or soup.find("div", {"class": "block-body"})

    if detail_div:
        raw_text = detail_div.get_text("\n", strip=True)
        desc, specs = split_desc_specs(raw_text)
        result["description"] = desc
        result["specifications"] = specs

        # æŠ“æ‰€æœ‰å…§å®¹åœ–
        for img in detail_div.find_all("img"):
            src = img.get("src") or ""
            if not src:
                continue

            # åªæŠ“çœ‹èµ·ä¾†åƒå…§å®¹åœ–çš„ï¼ˆåå¥½ uploadsï¼‰
            if "uploads" not in src and "Upload" not in src:
                continue

            full_img_url = urljoin(BASE_URL, src)
            fn = download_image(full_img_url)
            if fn and fn not in images:
                images.append(fn)
                print(f"  âœ… è©³æƒ…åœ–: {fn}")

        print(f"  ğŸ“ æè¿°é•·åº¦: {len(result['description'])} å­—å…ƒ")
    else:
        print("  âš ï¸  æœªæ‰¾åˆ°ç”¢å“è©³æƒ…å€åŸŸ")

    # å»é‡ä¿åº
    seen = set()
    uniq = []
    for fn in images:
        if fn in seen:
            continue
        seen.add(fn)
        uniq.append(fn)

    result["images"] = uniq
    print(f"  ğŸ“Š åœ–ç‰‡æ•¸é‡: {len(uniq)}")

    return result


def update_data_json():
    if not os.path.exists(DATA_FILE):
        print(f"âŒ éŒ¯èª¤: æ‰¾ä¸åˆ° {DATA_FILE}")
        return

    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    products = data.get("products", [])
    print(f"\n{'='*60}\nå…±æœ‰ {len(products)} å€‹å•†å“éœ€è¦è™•ç†\n{'='*60}\n")

    processed_urls = set()
    updated_count = 0
    total_new_images = 0

    for i, product in enumerate(products):
        url = product.get("url")
        if not url:
            continue

        if url in processed_urls:
            print(f"[{i+1}/{len(products)}] â­ï¸  è·³éé‡è¤‡ URL")
            continue
        processed_urls.add(url)

        title = product.get("title") or "Unknown"
        print(f"\n{'='*60}\n[{i+1}/{len(products)}] å•†å“: {title}")

        old_images = product.get("images") or []
        if isinstance(old_images, str):
            try:
                old_images = json.loads(old_images)
            except Exception:
                old_images = []
        old_image_count = len(old_images)

        details = extract_and_download_all_images(url)
        if details:
            product["description"] = details["description"]
            product["images"] = details["images"]          # âœ… å­˜æª”å list
            product["specifications"] = details["specifications"]

            new_image_count = len(details["images"])
            if new_image_count > old_image_count:
                total_new_images += (new_image_count - old_image_count)

            updated_count += 1

            if updated_count % 5 == 0:
                with open(DATA_FILE, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print(f"  ğŸ’¾ å·²ä¿å­˜é€²åº¦ï¼ˆ{updated_count} å€‹å•†å“å·²æ›´æ–°ï¼‰")

        time.sleep(1)

    with open(DATA_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    total_images = sum(len(p.get("images") or []) for p in products)
    products_with_images = sum(1 for p in products if p.get("images"))

    print(f"\n{'='*60}")
    print("âœ… å®Œæˆï¼")
    print(f"  â€¢ æ›´æ–°å•†å“: {updated_count}")
    print(f"  â€¢ æ–°å¢åœ–ç‰‡: {total_new_images}")
    print(f"  â€¢ åœ–ç‰‡ç¸½æ•¸: {total_images}")
    print(f"  â€¢ æœ‰åœ–ç‰‡çš„å•†å“: {products_with_images}/{len(products)}")
    print(f"  â€¢ å¹³å‡æ¯å€‹å•†å“: {total_images/len(products):.1f} å¼µ")
    print(f"ğŸ“ è³‡æ–™å·²ä¿å­˜è‡³ {DATA_FILE}")
    print(f"{'='*60}")


if __name__ == "__main__":
    update_data_json()
